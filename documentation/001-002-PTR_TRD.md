# Technical Requirements Document: PTR-Based Tool Filtering System

## 1. Executive Summary

This document outlines the technical requirements for implementing a modified PTR (Precision-driven Tool Recommendation) system that filters MCP (Model Context Protocol) tools based on conversation context and historical usage patterns.

## 2. System Overview

### 2.1 Purpose
Build a tool recommendation system that:
- Analyzes conversation history between users and assistants
- Filters available MCP tools to recommend the most relevant subset
- Learns from historical usage patterns
- Improves recommendations over time

### 2.2 Core Components
1. **Message Parser**: Processes conversation chains (Claude/OpenAI format)
2. **Tool Registry**: Manages MCP tool definitions
3. **Historical Pattern Store**: Tracks tool usage patterns
4. **Semantic Engine**: Handles embeddings and similarity search
5. **PTR Engine**: Implements the three-stage recommendation algorithm
6. **Evaluation Framework**: Measures system performance
7. **API Layer**: Exposes functionality via REST/GraphQL

## 3. Functional Requirements

### 3.1 Input Processing
- **Conversation Parsing**
  - Support Claude and OpenAI message format
  - Extract user intents and context
  - Identify tool usage patterns in assistant responses
  
- **Tool Definition Management**
  - Parse MCP standard tool definitions
  - Store tool metadata (name, description, parameters)
  - Support dynamic tool registration/updates

### 3.2 PTR Implementation

#### Stage 1: Tool Bundle Acquisition
- Analyze historical tool combinations for similar queries
- Start with configurable baseline (0 for new users)
- Build user-specific and global usage patterns

#### Stage 2: Functional Coverage Mapping
- Decompose conversation intent into functional requirements
- Map requirements to available tools
- Identify coverage gaps

#### Stage 3: Multi-view Re-ranking
- **Semantic Alignment**: Match conversation context with tool descriptions
- **Historical Correlation**: Weight by past usage success
- **Contextual Expansion**: Consider complementary tools

### 3.3 Historical Learning
- Track tool usage per conversation
- Store success/failure signals
- Build user-specific profiles
- Aggregate global patterns

### 3.4 Output
- Return filtered list of recommended tools
- Include confidence scores
- Provide reasoning (optional)

## 4. Non-Functional Requirements

### 4.1 Performance
- Tool recommendation latency < 100ms
- Support 1000+ concurrent requests
- Handle tool sets up to 1000 tools

### 4.2 Scalability
- Horizontal scaling for API layer
- Distributed storage for historical data
- Cacheable embeddings

### 4.3 Reliability
- 99.9% uptime SLA
- Graceful degradation without historical data
- Fallback to rule-based filtering

### 4.4 Security
- API authentication (JWT/OAuth2)
- Data encryption at rest and in transit
- User data isolation

## 5. Technical Architecture

### 5.1 System Components

```
┌─────────────────┐     ┌──────────────────┐
│   API Gateway   │────▶│  Message Parser  │
└────────┬────────┘     └──────────────────┘
         │                        │
         │              ┌─────────▼────────┐
         │              │  Intent Analyzer  │
         │              └─────────┬────────┘
         │                        │
┌────────▼────────┐     ┌─────────▼────────┐
│  Tool Registry  │────▶│   PTR Engine     │
└─────────────────┘     └─────────┬────────┘
                                  │
┌─────────────────┐     ┌─────────▼────────┐
│ Historical Store│◀────│  Result Builder  │
└─────────────────┘     └──────────────────┘
                                  │
┌─────────────────┐               │
│ Semantic Engine │◀──────────────┘
└─────────────────┘
```

### 5.2 Technology Stack (Finalized)

#### Core Technologies
- **API Framework**: FastAPI (async performance, auto-documentation)
- **Vector Database**: Qdrant (high performance, easy distribution)
- **Embedding Service**: LiteLLM (unified interface for cloud providers)
- **Cache**: Redis (in-memory, proven at scale)
- **Evaluation**: RAGAS + Phoenix + MLflow

#### Embedding Strategy
- **Primary**: Voyage AI (voyage-2) for quality
- **Fallback**: OpenAI (text-embedding-3-small) for reliability
- **A/B Testing**: Cohere, Google via LiteLLM
- **No Local Models**: Cloud-only for consistency and performance

### 5.3 Data Models

#### Conversation Model
```json
{
  "conversation_id": "string",
  "messages": [
    {
      "role": "user|assistant",
      "content": "string",
      "timestamp": "ISO8601",
      "tool_calls": []
    }
  ]
}
```

#### Tool Model (MCP Standard)
```json
{
  "name": "string",
  "description": "string",
  "parameters": {
    "type": "object",
    "properties": {},
    "required": []
  }
}
```

#### Historical Pattern Model
```json
{
  "pattern_id": "string",
  "user_id": "string",
  "query_embedding": "vector",
  "tool_bundle": ["tool1", "tool2"],
  "success_rate": 0.95,
  "usage_count": 42,
  "last_used": "ISO8601"
}
```

### 5.3 Embedding Model Integration (via LiteLLM)

LiteLLM provides a unified interface for all cloud embedding providers, eliminating the need for custom abstractions.

#### Supported Providers
- OpenAI (text-embedding-3-small/large)
- Voyage AI (voyage-2, voyage-large-2)
- Cohere (embed-english-v3.0)
- Google/Gemini (textembedding-gecko)
- Amazon Bedrock (Titan embeddings)

#### Configuration Schema
```json
{
  "embedding_config": {
    "primary_model": "voyage-2",
    "fallback_model": "text-embedding-3-small",
    "cache_ttl": 3600,
    "batch_size": 100
  }
}
```

### 5.4 API Specification

#### Endpoint: Filter Tools
```
POST /api/v1/tools/filter
```

Request:
```json
{
  "conversation": {
    "messages": []
  },
  "available_tools": [],
  "user_id": "string",
  "options": {
    "max_tools": 10,
    "include_reasoning": true
  }
}
```

Response:
```json
{
  "recommended_tools": [
    {
      "tool_name": "string",
      "confidence": 0.95,
      "reasoning": "string"
    }
  ],
  "metadata": {
    "processing_time_ms": 42,
    "patterns_used": 3
  }
}
```

## 6. Data Storage Requirements

### 6.1 Conversation History
- Time-series data
- Retention policy: 90 days default
- Partitioned by user_id

### 6.2 Tool Registry
- Key-value store
- Version history
- Change notifications

### 6.3 Historical Patterns
- Vector database for embeddings
- Relational DB for metadata
- Redis for hot cache

### 6.4 Embeddings
- Pre-computed for tools
- On-demand for queries
- Cached with TTL

## 7. Integration Points

### 7.1 LLM Integration
- Embedding generation
- Intent extraction
- Fallback reasoning

### 7.2 Message Format Support
- Claude API format parser
- OpenAI API format parser
- Extensible parser interface

### 7.3 MCP Tool Integration
- Tool definition ingestion
- Dynamic tool updates
- Validation framework

## 8. Evaluation Framework (Finalized)

### 8.1 Evaluation Stack
- **RAGAS**: Core retrieval metrics (precision, recall, relevance)
- **Phoenix by Arize**: Observability and debugging
- **MLflow**: Experiment tracking and A/B testing

### 8.2 Key Metrics
- **Precision@K**: % of top K recommended tools that were used
- **Recall@K**: % of used tools that appeared in top K
- **NDCG**: Normalized Discounted Cumulative Gain for ranking quality
- **MRR**: Mean Reciprocal Rank of first relevant tool
- **Latency**: P50, P95, P99 response times
- **F1 Score**: Harmonic mean of precision and recall

### 8.3 Test Scenarios
- Cold start (no history)
- Warm start (rich history)
- Edge cases (ambiguous queries)
- Different tool set sizes (10, 100, 1000 tools)
- Multi-language queries

### 8.4 A/B Testing Strategy
- Embedding model comparison (Voyage vs OpenAI vs Cohere)
- Algorithm variations (semantic only vs full PTR)
- Threshold tuning experiments

## 9. Development Phases

### Phase 1: MVP
- Basic message parsing
- Simple semantic matching
- In-memory storage
- REST API

### Phase 2: Historical Learning
- Pattern storage
- User profiles
- Improved ranking

### Phase 3: Production Ready
- Distributed architecture
- Performance optimization
- Monitoring and alerting

### Phase 4: Advanced Features
- Real-time learning
- Cross-user patterns
- Explainable AI

## 10. Success Criteria

1. **Accuracy**: >80% of recommended tools are used
2. **Efficiency**: Reduce tool set by >50% on average
3. **Performance**: <100ms P95 latency
4. **Adoption**: >90% of requests use filtering

## 11. Risks and Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| Cold start problem | Low accuracy for new users | Configurable baseline, global patterns |
| Embedding quality | Poor semantic matching | Multiple embedding models, evaluation |
| Scale limitations | Performance degradation | Caching, horizontal scaling |
| Privacy concerns | User trust | Data isolation, encryption |

## 12. Open Questions and Decisions

See accompanying TODO list for technical decisions to be made.